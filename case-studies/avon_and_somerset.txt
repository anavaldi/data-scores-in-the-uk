Avon & Somerset Police Qlik Sense
Summary
Qlik Sense was first piloted by Avon & Somerset Police in 2016 and now has over 30 applications across teams. It serves as both a performance assessment tool and a predictive policing tool. Developed in part as a response to on-going austerity measures, the system is a form of self-service analytics software that connects internal datasets within Avon & Somerset Police, as well as some datasets from other agencies in Bristol Council, to provide integrated assessments and evaluations. The focus in this report is the predictive modeling for individual offenders and victims as well as neighbourhood mapping of crime. Built into Qlik Sense applications are offender risk scores and vulnerability risk scores along with a harm rating that determines an overall risk. It is intended as a ‘one-click’ system that provides individual offending and intelligence profiles to help ‘triage’ risks and threats. The system is used by frontline staff to decide on allocation of resources and pathways of managing highest risk offenders. In some instances, such as domestic abuse, it is used to decide on who to manage and to enable pre-emptive measures.   
Implementation
Qlik Sense was piloted by Avon & Somerset Police in 2016 and put into use in January 2017 across different parts of the police force. In interviews it was noted that the decision to introduce Qlik Sense in Avon & Somerset Police work came in the context of on-going austerity measures, with around £80 million cuts in Avon & Somerset Constabulary, and attention towards developments in technology amongst the leadership team. One previous manager, who worked on developing the system within the police force said, ‘there became an opportunity for Avon and Somerset to say, actually we don’t want to keep having to do things like we used to do it because we’re just not going to be able to survive. There’s a tipping point in the organization and we have to do something to enable to do things differently in the world, like the modern world’s doing.’ (former employee) He went on to note, ‘it’s viewed very much as a critical enabler, strategic imperative for any…organization that’s facing cuts.’ 
Qlik Sense noted in a press release relating to the contract with Avon & Somerset Constabulary that the analytics platform is used ‘to visualize its command center operations data to gain better insight into the availability, objectives, and location of its police officers against public demand.’ It started as a management tool, collecting performance data in relation to staff, such as ‘how many crimes they’re managing, whether they’re contacting victims, whether those crimes are being reviewed by sergeants.’ (chief inspector) From that it ‘mushroomed’ and ‘now It’s really the data help around which everything revolves.’ (chief inspector) Whilst it initially developed to visualize data in 12 apps, it was noted in interviews that Qlik Sense now has over 30 apps in use by Avon & Somerset Police for different functions and by different teams, and has around 4000 licenses issued across frontline staff. This includes the offender management app that uses predictive modeling and profiling for offenders, including level of risk, cohort and crime pattern. It was noted in an interview that there are about 250,000 offenders within the Avon & Somerset area that are given a score (coordinator).
The system builds on previous predictive modeling software developed by Avon & Somerset Constabulary using IBM Predictive Analytics for tackling domestic violence and child abuse in which risk scores are produced based on ‘historical crime data, along with textural and sentiment analysis combined with additional databases and open-source information, to create a statistical model that can predict an individual’s behaviour and risk…allowing officers to identify potential victims before they are harmed.’
Model
Qlik Sense was described in interviews as a form of ‘self-service analytics software’ in which different officers can access different parts of the system to locate information relevant to them. The data that informs dashboards and predictive modeling was summarized in an interview as being ‘primarily internal datasets, so that is our call handling data, our crime intelligence data, missing people data, our command and control data and obviously our HR and finance and more backend datasets, operational data like airwave data so the GPS pings from officers, sourcing from user satisfaction survey data and internal staff survey data.’ (manager) Currently, this might be expanded through further data sharing between agencies and the use of partner data (such as Council held data) using Home Office Transformation funding as well as information sharing with voluntary agencies (chief inspector). In one interview, it was also noted that for aspects of predictive modeling further external data is used like, ‘weather models, weather forecasting stuff coming in to influence crime trend and patterns.’ (former employee) (see dashboard image below) 
It was also noted in the same interview that ‘they do use also some social demographic information, like the Acorn type information that comes in, and that’s used by looking at areas of high crime rates, deprivation and looking to be able to support any outliers or any areas which need different sorts of interventions to help spot that.’ (former employee) However, it was noted in a different interview that Avon & Somerset police do not use demographic data such as ethnicity as part of the predictive modeling of individuals or third party modeled data, such as that provided by Mosaic or Acorn.
With the police-held data, offender risk scores and vulnerability risk scores are produced. The offender risk score will be a percentage score between 0 and 100 that identifies ‘the likelihood of offending’ in combination with the harm that an offender carries: ‘So if it’s an offender that’s previously done rapes or GBHs [Grievous Bodily Harm] or attempted murders or threats to kill, a number of things, this harm rating, combined with the likelihood of offending, allows you to determine overall risk for that offender.’ (former employee) That means that one of the variables that are weighted are type of offence: ‘it’s weighted for people who are going to commit a violence offence, [they] are scored higher, at more risk.’ (inspector) A section of the model identifies the risk of committing a serious domestic abuse offence which is based on data about previous offenders of domestic abuse: ‘we put in all data of, say, somebody that has already committed a domestic abuse offence and we put in their characteristics. So this percentage is how many of those characteristics that person fits.’ (inspector) The model also includes an escalation risk: ‘once you’re measuring risk in an automated way, you can then measure the escalation risk. So if someone’s offending behavior changes over the last week or two or even overnight, the model will then show you that and it’ll push it up the list.’ (former employee) In that way it distinguishes between risk associated with previous offences and escalating risk associated with recent or current offences: ‘So it enables you to, if you have some that are a very similar score, it decides…which one of those is escalating. So that means that their offending behaviour is happening now.’ (coordinator) 
The vulnerability risk score is modeled with a similar approach to the offender risk score, providing a percentage score for the likelihood of an individual becoming a victim of crime based on data such as ‘if you’ve been a victim of crime but also personal antisocial behaviour and missing persons incidents.’ (manager) It was noted in interviews that parts of the system are linked up with Bristol’s Integrated Analytical Hub (see case study in this report) ‘to help, in Avon and Somerset Police’s case, to determine potential long term missing people’ (former employee) as one aspect of vulnerability. However, it was also mentioned that the accuracy rate for offenders is more accurate than it is for vulnerable people (coordinator). 
Deployment and Uses
Qlik Sense is a suite of analytics applications of which predictive modeling is just one aspect of the system. In interviews it was noted that analytics feeds into ‘everything from a strategic level’ (former employee) including strategic decision making for the organization and governance of the organization and management. There are continuous analyses fed to different tasking processes; what was described as a ‘24/7 live cell’ in which analyses go out to the organization: ‘here’s our top wanted offenders, that goes out to local teams, neighbourhood teams, [to those with] responsibility to manage offenders.’ (former employee) It is used for crime trends daily, such as mapping burglary crime trends in relation to what there would ‘normally’ have been of those crimes. A key feature of the system is therefore to inform police ‘in relation to demand’ and to use it ‘to decide which people we are going to go after out of this big list [of people who are wanted].’ (coordinator) 
Individual risk scores are used to alert officers to different forms of risk. Police officers are provided with some guidance as to how risk scores have been calculated, which can help them make sense of it. Including this aspect in the interface of the dashboard is important as it was noted in interviews that there is an emphasis that the model is not driving decision-making, but that professional judgement is: ‘it is just a tool so we wouldn’t just go down the route of saying that person’s the highest score, they’re the most risky people, we also look at here and now. So if there’s a job that comes in and someone’s, say, threatened someone with a firearm, then there’s always a professional judgment that we use around the experience of people that work within our unit.’ (coordinator) This framing has been key to its deployment with front-line staff. One inspector explained the nature of the initial hesitancy around the model: 
When we first started looking at it, we couldn’t understand it at all. It’s like a new thing to us that’s saying person A is more risky than person B and our knowledge is no, actually person B is more risky. So we had a whole discussion with the Qlik Sense team about what goes into it and what doesn’t. Now for us, when we’re managing people, we look at nine pathways of their life that could help them turn away from offending. So you look at accommodation needs, their drugs and alcohol needs, their mental health or physical health, children and families, finances and there’s some specific pathways for women and sex workers…we monitor people so closely that the slightest thing like the breakdown of a relationship could cause them to reoffend. Now that breakdown in the relationship isn’t going to go into Qlik Sense because it’s not a crime, it’s an intelligence report and Qlik Sense doesn’t pick up intelligence. So we were quite frustrated by that at the beginning. (inspector)
She went on to note: ‘once we accepted … that it wasn’t the be-all-and-end-all but it was a tool, then it became much more effective for us.’ (inspector) In another interview it was pointed out: ‘what the model’s doing is very quickly saying hey, there may be some risk here but that’s for that practitioner or that local team, it’s them that make the informed decisions of who they’re going to prioritise and what we’re going to resource, and not the model.’ (manager) In this context, the system was referred to as a ‘triangulation tool’ used in addition to referrals and intelligence reports, and there ‘entirely to support a professional judgement’ (manager) 
In particular, it was noted that the model is used to provide an indication of individuals that might need management, and to assess what team would be best suited. One inspector explained the traffic light system they use for making this decision: 
we risk assess prolific acquisitive crime offenders using a traffic light system, so we have red, amber and green. The red offenders are managed by a specialist team and the amber offenders are managed by the neighbourhood team because they need less attention. So if we’re moving someone from red to amber because they are doing well, that gives us an opportunity to bring on another red one, the team should be looking at QS and saying who’s scoring high, do we need to scope them for consideration of them coming onto our scheme? (inspector) 
In that sense, the use of the system was explained as being about ‘focusing us in the right direction’ (inspector), particularly in a context of limited resources. In practice, this means that the system ‘highlights it to you that actually you need to get this person into custody sooner rather than later.’ (coordinator) In cases of domestic abuse, the system has been used to identify the top 15 offenders who are most at risk of committing a serious abuse offence based on matching characteristics, and engaging other agencies and teams on that basis as part of pre-emptive measures. The system now also tracks the pathways of management for offenders in order to be able to in future evaluate the ‘success’ of measures taken for lowering risk scores of those people being managed by a particular team.  
Moving away from individual risk, Qlik Sense is also used ‘to forecast demand, predict demand’ and to identify ‘command and control incidents that haven’t currently been classed as a recordable crime but have all the attributes that they should be.’ (manager) 
Auditing and Safeguards
It was noted in interviews that the Avon & Somerset Constabulary leadership carried out some consultations with the local community through council meetings in addition to public engagement through media when implementing Qlik Sense. In addition, it was noted that users were surveyed in terms of the proportions that are using it and what they think of it and that when the industry watchdog HMIC visits Qlik Sense ‘will typically get mentioned. So they’ll go out and check and test the reality of how it’s being used on the ground in these different use cases.’ (manager) 
The system also went through initial testing ‘about the individuals that were being identified and having a bit of a check and test on whether these individuals are the right type of individuals. So we went through all of that process and the best top evaluation of the model accuracy, the precision of the models, the recall of the models.’ (manager) The Qlik software is updated and changed every ten weeks.
Finally, usage is all audited, ‘what people are using, where, when, what and all that kind of stuff. It’s also designed in such a way where people are only given the information they need to use and know and that kind of thing.’ (former employee)
Challenges
It was noted in interviews that because of previous failures with the implementation of new technologies, a key cultural challenge is that ‘a lot of confidence around technology is low.’ (former employee) From management’s perspective, this relates to a challenge with ‘data literacy’ that means it is ‘working hard on data literacy in terms of people’s confidence and ability to engage, interpret and argue data and placing data at the centre point of how people make decisions.’ (manager) This was also emphasized in another interview in relation to an unwillingness to share information: ‘I think what we’ve got to do is really, at the same time as pushing out that analytics kind of culture, is really also promote the data literacy culture at the same time as well…because often you find the biggest issues and cases you get is where people don’t use information or don’t share information and that’s often where we get the problems.’ (former employee) This also means being more ‘proportionate’ about sharing information: ‘I think if we’re expected to continue to reduce our budgets, continue to battle against austerity, continue to make people safer in a way that technology can help us do that, we’ve just got to be a little bit more proportionate to that…How much do you hear really of information going astray to do terrible things when it’s with the police? It’s really limited, isn’t it?’ (former employee) 
However, at the same time, amongst frontline staff, a key challenge that was noted in interviews is that too much importance is attributed to the system over and above professional judgement: ‘Challenges-wise, I think it’s that there are still some people in the organization who believe it is the be-all-and-end-all and professional judgement isn’t quite as important. So there will still be people who say…this is what we must do.’ (inspector) This was, in part, explained by the question of being able to defend decisions in relation to the model: 
If somebody is shown on our system as being a really high risk of committing, for example, a domestic abuse offence and we say actually that’s person A, person B over here we believe is higher risk, even though their [person A] score is higher. So we do something with person B and we don’t do something with person A and then person A then goes on to kill someone or seriously injure them, where is the defenceability around that? So I can understand people’s thinking in that sense, I really can. (inspector) 
In this context, the onus then falls on the frontline staff to record or explain any decision they make that might be at odds with what the system is telling them. In a different interview, the related issue of ‘dependency’ was highlighted as a potential concern with the system: ‘You become reliant on any system and when it goes down, it crashes, then we can still work, it’s not going to stop us working but you can become too dependent on technology sometimes.’ (coordinator)
This becomes pertinent in relation to the model’s accuracy and the challenge of data quality: ‘the model doesn’t get it right every time’ and when it comes to data quality issues, ‘we use record management systems that aren’t always completed or as accurate as we would like’ (manager). In another interview it was similarly noted that being able to interrogate risk scores is important in light of accuracy questions: ‘if someone has got a particularly high score, we will look at what’s given them the high score and drill in to make sure the data’s correct but it isn’t always. For example, it might be a data quality issue where someone is identified as high risk because they were previously linked to a murder or attempted murder and actually they were eliminated from that murder.’ (coordinator) In response to this, Avon & Somerset police have developed an app to try and capture data quality gaps, ‘right down to an individual officer level. So we can track data quality issues over time.’ (manager) In one interview it was outlined how this has led to an emphasis on ‘personal responsibility’ with regards to ‘the quality of the information going in’: ‘we’ve got thousands of staff inputting records and training and all of these things can lead to data errors. So what we’ve done is we’ve utilised Qlik as a system to pull out those errors and actually put them in the officers’ domain.’ (chief inspector) 
A final challenge highlighted in interviews is the ability to actually act on the risks visualised in the system. Whilst the system identifies different aspects, it is another question what plans are in place to manage those different aspects, something that was noted as still being a process (‘we’re getting there’) that relate on ‘the changeover from not having the system before and people getting used to putting things in place.’ (coordinator) 


