Bristol Integrated Analytical Hub
Summary
The Integrated Analytical Hub is an in-house developed system that was established out of Bristol City Council’s Think Family approach to the national Troubled Families programme ‘to encourage services to deal with families as a whole, rather than responding to each problem, or person, separately’. The Troubled Families programme was launched in 2011 to help families who struggle with factors such as unemployment, crime and poor school attendance. Think Family identified families facing issues, such as parents and children involved in crime or anti-social behaviour; children not attending school regularly; children who need help; adult out of work or at risk of financial exclusion and young people at risk of worklessness; families affected by domestic violence and abuse; parents and children with a range of health problems. Bristol’s Think Family programme is now in its second phase. As a result of the learning from the first phase, Bristol developed a Think Family Database that consolidated information from 35 different social issue datasets, about 54,000 families in the local authority area, to understand the strategic and operational needs of the city. The database is up and running and is now able to provide information to predict future need as well as simply responding to presenting issues. 
Implementation
The Integrated Analytical Hub is developed in-house by staff employed by Bristol City Council. This was explained as being about having ‘complete control over everything’ and a concern that ‘if you get somebody else with a black box, no one really knows how it works. Assuming you’ve got to explain it to somebody, you’ve got no chance.’ (data scientist) The decision to develop it in-house also came from a pragmatic concern with using an iterative model based on ‘the existing IT’ and that ‘wouldn’t have high level maintenance costs going into the future’ (manager).  
Initially, the hub was created as a ‘data warehouse’ around the Troubled Families programme, that combines data of ‘all social issues’ across the city for children and families to provide a ‘holistic understanding’ of the family (manager).  The decision to create a data warehouse came from a perceived need to ‘have a more strategic understanding of the city, the challenges the city faced, the families faced and understand that…[to] make better strategic plans, make better resource decisions, analyse the information better to understand where the risk and vulnerability was in the city, and who was working with those people.’ (manager) 
Once the data warehouse was created, the team began looking into doing ‘more interesting things’ such as predictive modeling, particularly in relation to Child Sexual Exploitation (CSE). In a document from Bristol City Council obtained as part of an FOI request, the CSE Model is reasoned as: 
Predictive modeling allows us to make better use of data, to understand the known issues affecting the citizens of Bristol now and in turn how these factors may develop in the future. Understanding future trends can inform resource allocation and decision making on both strategic and operational levels.
As stated by a senior manager, the ability to do predictive analytics relies on having created a data warehouse first that combines data for all children and families around social issues: 
I thought actually what you could do is start to use predictive analytics around social issues. So could you look at the wider factors that are supporting the reason why a child or young person would demonstrate certain behaviours and understand those characteristics and then start to look at that from that perspective. (manager)
This requires an understanding of characteristics within the cohort who demonstrate the behaviour in relation to the wider population. ‘So in order to do that predictive analytics, you needed a data warehouse of all the social issues.’ (manager) Moreover, the predictive model is a way to overcome discretion and reliance on professional judgement only: ‘it was self-evident and common-sense that there were certain social situations that were precursors to demonstrating future types of behaviour and whilst it was self-evident to the professional…what we didn’t have was any of that on any scientific footing.’ (manager)
Model
The cycle shown in the diagram represents steps in developing a predictive model. It highlights the importance of testing and the cyclical nature of the process following feedback from the business (obtained through FOI request). This is informed by what is described as ‘co-design’. In the case of the Bristol CSE model, partners from the Barnados project BASE (Barnardo’s Against Sexual Exploitation) were consulted while gathering evidence and information on known cases of CSE in Bristol.
The model is based on 35 different social issue data-sets (two of which are not currently collected, see below).

In interviews it was noted that the Council also buys some data in from  external companies such as Experian, for a Joint Strategic Needs Assessment (JSNA), which is a needs analysis based on ward based area data ‘to look at population growth and various things.’ (manager) The system is running on a combination of software, including SSRS for front line staff accessing the system, SPSS for predictive modeling and Qlik Sense for MI and analytical functions. The model produces an automated risk score from 0 to 100 for every young person in the database. This score is based on statistical probability of similarity in relation to characteristics from a training cohort. This training cohort consists of confirmed victims of exploitation over the last 2-3 years that have been profiled in order to assess how statistically similar young people in the database are to this training cohort. This process was described in an interview as being a form of ‘mirroring’: ‘it’s not predicting you will be sexually exploited or whatever, it’s saying you are demonstrating exactly the same characteristics and behaviours as someone who was sexually exploited.’ (manager) 
The model is now integrating more real-time data into the score produced, by connecting to universal services and receiving daily data on factors such as school attendance: 
at midday we’ll be able to see if somebody attended school this morning…it’s a lot more useful because it’s got some value to it whereas social workers were, why are you telling me someone didn’t attend school in December? That’s not interesting for me because I need to know what they’re doing now. (data scientist)
Importantly, the model does not account for any ‘insulating factors’ but will only account for ‘negative’ data, or information that might increase risk, such as school attendance, records of domestic abuse, etc. This means that it relies on the professional case worker to account for other information or more contextual data not captured in the model, such as the person being actively engaged with social groups despite not attending schools or having a strong protective network around them outside the immediate family. As explained in one of our interviews, ‘that’s where we say it’s down to the lead professional to say it looks like this person is at risk and they’re very similar to one of our victims from last year but you know them better than we do, now make an informed decision around what you’re going to do on that.’ (data scientist) They went on to note, ‘there’s only so far you can go with data, I think’ (data scientist). 
Deployment and Uses
In interviews it was stated that about 450 frontline workers have access to the Integrated Analytical Hub with about 150 regular users, using different bits of the system. For the data warehouse, the system provides an overview of a family based on what is known about them across the public sector, where the data comes from and who is working with them: ‘the idea is to try and break down some barriers.’ (data scientist) The First Response team is using the data warehouse for deciding where to send families who engage with services, based on the overview provided in the database: ‘we’ve got far too much traffic coming through which is overwhelming and if we can work out a way in which better decision-making could be made, then we will use our resources more wisely.’ (manager) This has required ‘a lot of work on trying to understand what a family actually is and how to group people together and understanding their needs.’ (data scientist) It has also required that information is shared across public services. A prominent area where this is not happening in Bristol is in relation to health data, although it was noted in an interview that this is changing as health professionals are becoming more interested in social issues for health-related concerns. 
In terms of the predictive modeling aspect for CSE, it was noted in interviews that this is more commonly referred to as ‘targeted interventions and targeted risk assessments’: ‘Quite often I don’t talk about predictive analytics…People get a little bit uncomfortable about the word predictive.’ (manager) Emphasised in both documents and in the interviews is the need for continued judgement from a lead professional in using predictive modeling and interpreting risk scores. One documented obtained from an FOI request stated: 
Predictive analytics should be interpreted intelligently, the results of a model do not replace a lead professional’s assessment nor are the outcomes generated guaranteed. Instead the results are meant to be used as a tool to get ahead of the curve, this use of data supports an early intervention approach.
This was supported in interviews: ‘it’s not computer says yes or no, it’s computer provides advice and guidance to the professional who adds the professional judgement in order to make better decisions about resource allocation.’ (manager) Workshops and one-to-one sessions, ‘upskilling staff to understand what [the model] means and how they can use it’ (data scientist) was highlighted as an important aspect of the training for implementing the system amongst frontline staff. Especially as it was explained that not many people have knowledge of how the model works or the methodology of scoring. It was, however, noted that the system provides a ‘context paragraph’ next to the score: ‘So they’ll see this person is scored whatever and then they get the reason why.’ (data scientist) 
In terms of how the system is acted upon, a document obtained through an FOI request states: 
Using the model to identify children and young people with the most heightened risk scores, we have allocated 243 cases to key workers over the last year. As a result these families have received targeted support. 
In interviews it was explained that if a child or young person has a named case coordinator, information will be sent to them that the individual has been flagged in the system, and will request information on what activity is being done to safeguard the individual, and for this to be recorded in the case notes. In cases where the child or young person is not currently receiving any support at all, someone from the commissioning team would be encouraged to ‘go out and try and a proactive piece and engage that family.’ (data scientist) The level of risk will also inform what team should be dealing with the case. As explained in an interview, ‘If you identify, say, high, medium and low risk, we’ve got different services that deal with high, medium and low. So our really high safeguarding needs will go straight into our social work teams. Where there’s child protection issues, we’ve also got a targeted youth contact which is about helping young people, signposting, self-esteem, confidence, lower level social issues and you would find your way to the right service more quickly.’  (manager) 
Whilst the predictive model has attempted to focus on CSE, it was noted in interviews that the Council is moving towards a wider focus that looks at exploitation more generally, creating effectively a ‘vulnerability index’: ‘What I don’t think it actually tells us is that vulnerability will play out in that you will be sexually exploited because it may be that you will be criminally exploited or you will go onto offend or you will become a drug addict.’ (manager) The aim is to rebuild the model to take account of these different forms of vulnerability to create a kind of ‘spidergram’ outlining ‘a vulnerability index with a propensity towards one thing or the other.’ (manager) 
In interviews it was noted that the model is now also being used for higher-level analysis, identifying trends, patterns, geographical areas, to identify where the most risk is held. An example given was to try and identify what schools have the greatest need by aggregating all the risk scores produced: ‘we do everything at a person level and I think that’s the only way it has any value, but in saying that, once you’ve got the fine granular level, then you can always add back up again.’ (data scientist) 
Auditing and Safeguards
The database is updated every week with an accompanying risk analysis. An accuracy measure will be generated each week, comparing those individuals in the target cohort to those the model has flagged. This measure will be subject to a particular threshold of accuracy: ‘what it does is it automatically tells you and alerts us once [the accuracy measure] drops below a certain threshold and says it’s no longer as good a predictor as it was six months ago, you need to rebuild it.’ (data scientist) The accuracy measure is based on a combination of ‘precision’ and ‘recall’. In documents obtained from an FOI request, these methods were outlined as:
Use of the system is recorded, including capturing key touches to audit what has been done on the system. This includes a trigger mechanism on volume of searches that requires a response to explain why the search has been done. 
In terms of consultation or evaluation of how the system informs decision-making by frontline staff, this has not been done. This was explained in interviews as being to do with resources and ability. There also has not been any consultation with service users or an assessment of the effectiveness of targeted and early intervention. Such an assessment, it was noted, would be based on ‘re-referral rates’ (data scientist). 
The data used has been shared using ‘other arms of the Data Protection Act where we’ve got statutory duty’ rather than consent: ‘we put together all of this statutory legislation that places a duty upon us and having checked all that through, we’re comfortable that in order to fulfill those statutory duties to a reasonable level, we can share this information.’ (manager)  
Challenges
Several challenges with implementing and using the system was noted in interviews. They predominantly concern technical and cultural challenges. From a technical perspective, two key challenges that were emphasised are data quality and noise. Some data-sets have a high volume of errors, for example those relating to arrest records ‘with people giving wrong names, wrong date of birth, things like that’ (data scientist). There have been attempts to account for that by weighting more reliable data-sets more highly in the system, for example information on unemployment from the Department for Works and Pensions or housing applications where people are more likely to provide correct information. Yet data can also have been recorded incorrectly and, it was noted in an interview, mistakes in the host system of any given data-set will reverberate all the way through the system. This is a problem as all the different data-sets are owned by different parts of the Council and cannot be corrected by the team behind the Integrated Analytical Hub: ‘we can see data quality problems, we can’t fix them. We don’t own any of the systems that we report from.’ (data scientist) The other concern that was expressed is a lack of filtering of data. The system is based on collecting as much data as possible and cleaned data cannot be passed back to the original data-set used as a source: ‘everything we do is pulling, we don’t ever push back.’ (data scientist) 
The other key type of challenges concerns the culture of the workplace and of the professionals working in social services. In the first instance, this concerned the challenge of getting workers to use the system, where trepidation was explained as being in part to do with a lack of technical skills and a skepticism about changing the knowledge production regarding individuals: ‘There’s been a strongly held view that the only people who should tell you something about them is children and families themselves.’ (manager) 
In terms of those using the system, the main challenge expressed in interviews concerns the nature of interpretation and change of work practices that come about from using the data warehouse and the predictive model:
we can’t control what people do off the back of it…they might misconstrue, over-worry. It might force them into activity they wouldn’t otherwise do and we don’t want to generate a whole lot of concerns and worry and stress that doesn’t really need to be there. (data scientist) 
This was further explained as being in part to do with how risk scores are presented to workers: ‘I think there’s a risk that once something’s in a system and somebody sees it and it’s got a number next to it, they think it must be right because the computer’s told me that and then they just forget all of their professional knowledge and judgement and say the computer says this.’ (data scientist) There have been attempts to try and address this by deliberately not using colours like red, amber and green, or to name something ‘high risk’ on the system. Trying to address interpretation issues is seen as very important as a way to mitigate the risk of feedback loops emerging from actions taken on scores (for example, alerting police because of a high risk score, in which contact with the police is a factor for generating a higher risk score): ‘You’ve got to be careful you don’t end up generating some feedback loops where your scores feeds another score feeds your score, and you end up just constantly multiplying everybody’s score each week. There’s definitely a risk of that.’ (data scientist) 
Related to this is the challenge of shifting to ‘preventative proactive work’ amongst workers who are ‘used to responding to high levels of need in crisis’ (data scientist). ‘Capturing more risk’ through the use of automated risk assessments will require engagement with individuals who are not usually considered high enough need, asking for a ‘light touch’ that engages with people on an on-going basis. Expanding the pool of risk through automated risk assessments also introduces issues in a context of austerity and limited resources. At one level, this was noted as a reason for some hesitancy around the implementation amongst some workers: ‘There’s been elements of why do you want to go and work out the risk and vulnerability because we’ve already got loads of people that are coming through the door with risk and vulnerabilities and what’s the point of finding even more people?’ (manager) On another level, it also introduces ethical challenges with regards to how you then decide whom to work with: ‘we have an internal ethics challenge board where we submitted some questions to them to say I’ve got a list of 100 people, I can only work with ten, how do I pick the right ones? Or what do we do with the ones that don’t receive any support because we’ve identified that there is a need there but we can’t resource it, so what should we do?’ (data scientist) In interviews it was noted that the approach so far has been based on being able to provide a justification for the decision to work with one person and not another. 



