Hackney’s Early Help 
Profiling System

Summary
Hackney County Council makes use of data analytics in a number of ways ranging from population level health analytics, fraud detection, to child welfare. This summary focuses on uses of data analytics in Hackney for child welfare. In child welfare, Hackney County Council is working with Ernst & Young (EY) and Xantura on the use of a system to identify children at risk of maltreatment and families who need additional support. The system is called the Early Help Profiling System. It has been funded by EY and London Councils. The system is being trialed and alerts have already led to early interventions.
Implementation
Xantura developed a tool they refer to as a Fusion platform “to better help services and reduce financial pressures in several business domains, including children’s services, adult social care and health, housing / homelessness and community safety’. Different local authorities in the UK are using Xantura systems to address different areas. In Hackney, the the Early Help Profiling System (EHPS) is being used to identify children at risk of neglect or abuse. The system uses ‘a predictive risk model which brings together data from multiple agencies to identify children who are most at risk of neglect or abuse’. The system is designed to provide social workers with monthly risk profiles that amalgamate information about families identified as most in need of early intervention. 
‘The project that we’ve been undertaking gives us an opportunity to pick out of the 54,000 plus children who live in Hackney those children who look most likely to benefit from early help service. The system is looking to identify children and families at an earlier stage before they would get to the point of seeing social workers and to direct them to different forms of support that will help them to alleviate their problems’. (local authority) 
The developer of the system says it has been designed not to be punitive, but to enable earlier intervention to prevent the need for statutory intervention. The system only shares data about people who are already working with an agency or professional. The alert is sent to professionals / case workers who make decisions about the kind(s) of services or interventions needed. The goal, according to those involved, is not to replace professional staff but to support them by giving them the information they need to do their job better (developers).
Across interviews, presentations and documents several contextual factors are presented as leading to the development and use of the EHPS.  As detailed across the case studies, there is recognition here too that local authorities are facing a funding crisis at the same time that demands for social supports are rising. EY and Xantura argue that they can help public sector agencies use advanced analytics to improve service outcomes, reduce demand for interventions and by doing so reduce costs.

The platform generally has also been described as addressing some problems in the social care system, particularly the challenges of sharing information about families.

In an interview developers referenced scandals such as those surrounding the cases of Baby P, Victoria Climbié and Fiona Pilkington as raising concerns about the failure of agencies to share and act on information. These cases, say a developer, point to the need to balance privacy rights with the rights of vulnerable individuals. The system has been developed to do this by using pseudonymised data, but making data identifiable to the professionals involved when an alert is generated by the model indicating a high risk threshold has been passed (developer). 

Model
The predictive models developed for local authorities, as with EHPS in Hackney, are based on local data. The idea is to build predictive analytics around outcome indicators and given the differences in data by location to ensure that local data is used to develop the modeling. They modelled nine months behind an event:
We have spent the last 18 months building a language generation tool which takes all of the analytics, all the stats and everything else that we generate and produces a research report. The research report doesn’t talk about a score of high, medium or low, it talks about the cause for concern based on the rules that our clients have defined. The outputs if the process say; this is what’s happened, this is the reason that we’re showing you the data, this is the family composition, this is the attendance levels, this is the chronology of events that have happened for the family, the service interventions that have gone in. And then the statistical analysis is saying, where does the family sit with respect to exclusions and is it getting worse, is it getting better, are siblings affecting younger siblings in the household? We’re still tweaking and tuning, so it’s not finished yet, but the idea is that we’re trying to augment the process so when the social worker looks at that case, they can very quickly do, in a more accelerated fashion, the job they would normally do; not change the job. (developer)
The developers argue that the system will be able to more quickly look at and assess historical data than a case worker would be able to do. They also note that the system has been designed with the help of those working in this area.
To date there is little information available about how risk is calculated, in particular about how variables are weighted. The variables used for uses of predictive analytics in child welfare as tested in other countries has come under criticism because of the way bias can be embedded.  
We submitted a Freedom of Information Request to the London Borough of Hackney requesting details about multiple systems and were told that responding to our request would exceed the hourly limit councils are obliged to meet. We have submitted a narrower request and are waiting on that response. Another FOI request asked specifically for details about Hackney’s use of the Children’s Safeguarding Profiling System. In this request it was stated that the Council could not reveal system details including manuals or data sharing agreements because it would damage Xantura’s commercial interests:  
‘Xantura and London Borough of Hackney are working together to develop the system as development partners, but Xantura anticipates operating on a commercial basis. We believe that to reveal detailed workings of the system would be damaging to their commercial interests and, while the project is in pilot phase, of limited public use. We therefore believe that the public interest in seeing any operating manuals is outweighed by Xantura’s commercial interests and exempt this part of the request under Section 43 of the Freedom of Information Act (Information Management Team, LBO).
Documents, publications and promotional material note that multi-agency sources of data are used. Datasets listed include school attendance, exclusion data, housing association repairs, arrears data, police records on anti-social behaviour and domestic violence, names, addresses, dates of births, unique pupil numbers, children and adult social care, housing, debt, council tax, housing benefits and substance abuse data. 
The illustration above is used in an EY promotional video to demonstrate the kinds of data used in the system. The data is pseudonymized before it is processed.  A developer noted that some of the data, such as the substance abuse data, has only been made available because they are using pseudonymized data (developer): 
We have names and address repositories with time series associated with them so we can see what’s happening to households all the time, but this doesn’t include any sensitive data. We then have a pseudonymised data repository, which is all the sensitive data matched, including substance misuse data, which clients can use for research purposes - the key point is that this is not identifiable data. The data sharing protocol rules … are set up as the key controls that mean actually I can move now from using the data purely from an analytical perspective, understanding general trends and doing analysis and building the models, to a real-world scenario, where a contact / referral is made into the front door in the MASH (multi-agency safeguarding hub), depending on reason for the contact, again defined in discussion with clients, we can run the risk model. So the risk model doesn’t run all the time, the whole alerting process is only running when a certain set of circumstances are occurring. So it’s a very controlled release of data from that pseudonymised repository of data. (developer)
A London Ventures summary notes that ‘timely data from the vulnerable families and data from others with whom they are in contact, using mobile phone and web technology’ is also being used. No further details about how mobile phone and web data are used is available. 
When describing the system a developer noted that particular scenarios that have been and are being developed will lead to the risk model running and potentially an alert being sent to a caseworker.
In the children’s model, say we’ve got an exclusion – if an exclusion is for a child just misbehaving in school and gets sent home for half a day, then they wouldn’t have met the criteria and the risk model wouldn’t run. If they’ve got a pattern of exclusion behaviour and actually it’s accelerating and they punch a teacher, and this is combined with wider risk factors, for example recent youth offending or ASB activity, then the risk model would run. (developer)
In terms of accuracy it has been noted that ‘Over 80% of households in Hackney that have been identified most at risk by the model are at risk’. However, in order to judge the accuracy of the model it is also important to know how many people were wrongly identified as high risk when they are not. It is not clear how the implications for those wrongly identified as high risk are considered or what opportunity people have to challenge or remove from systems a high risk assessment that is wrong.
In terms of consent, those subjected to the system are not being informed that their data is being used. The argument is that releasing details may prejudice potential interventions and compromise the commercial interests of the company involved, Xantura. It’s also noted in the Privacy Impact Assessment that there is no option to opt-out of having data included. The public more generally has not been consulted about the development and use of this system.
Deployment and Uses
The ideas behind the development of the system were described in 2015 by Hamza Yusuf, then head of finance at the London Borough of Hackney: 
‘The premise is that academic research and the government’s Troubled Families Programme identify a number of risk factors, both distal and proximal, related to child maltreatment. These can include benefit receipt, a history of offending, poor educational attendance or issues with parental capacity. The model we aim to pilot will identify these risk factors in a given family and, where they present collectively, an alert will be sent to a children’s services practitioner or our multi-agency safeguarding hub to investigate further. The model will help us to identify children who are at risk of maltreatment and target our interventions more intelligently, to prevent escalation into statutory social care’.
After a family or person has been identified as benefiting from early help the idea is that the case worker uses information in the report to identify the range of issues affecting the family to provide more support (developers).
‘What quite often we see is there’s a very definite cause and effect. Someone is struggling in school and so there might be an educational officer that might be able to support them, but if that educational officer doesn’t know that there’s been a challenge with mum maybe and mum’s struggling financially or she’s just lost her job, what we can do is highlight to the lead professional that there is also a broader stress in the household’ (developer).
 
Xantura and the London Borough of Hackney are working together to develop the system, but Xantura expects to operate commercially. The EHPS is being promoted as a system that can help councils save significant amounts of money by preventing family situations from escalating to the point where children are taken into care. 
It has been suggested that the screening of children and families could become fully automated and that the data could be used to generate snapshots of families that could be used in referrals going forward. A pilot project with general practitioners to assist them with making referrals has also been noted.
Auditing and Safeguards
A Privacy Notice indicates that data is encrypted and held in a secure facility. It is also noted that ‘only those council employees who currently use similar data will have access to the information supplied by Xantura (and) therefore current council vetting and tracking procedures will apply. It is stated that access to identifiable data will be through ‘a secure application with user access controls’ to ensure that only authorized people can access the data. It is also noted that ‘identifiable personal data will only be made available if certain business rules (based on an assessment of risk and vulnerability) have been met.
Access to data and sharing of data is said to happen through data sharing protocols and that every time data access occurs it is logged. Developers noted they are building a tool for council information governance so that reports can be generated that indicate everyone who has used the data and for what purpose (developer).
Recent media coverage indicated that the Information Commissioner’s Office is looking into the use of children’s data in this case. There has been no public report made.
Evaluations of the system are said to be iterative and ongoing, with the ‘performance of the predictive aspects of the system being tested statistically and piloted in a live setting’. In an interview it was noted that at present there was not a capability at this time to evaluate the impact the system was having on service users.
Developers say they are able to validate their models, but that tracking the impact of their alerts and related changes are challenging because councils do not have the baselines needed for them to measure and track impact (developer):
Operationally the benefits case isn’t just about cost avoidance, for example, the natural language capability benefits case is about whether we can improve the quality and reduce the time it takes for a MASH worker to go from contact through to assessment close, because by gathering all this data and presenting it to the social worker it takes less time to compile this research. In order to track these benefits we’re now creating reports that allow us to track contact to close on average by source. (developer)
At the moment, there is no way of measuring if and how this new system is affecting those using these services, to gain their perspective on how effective the early interventions have been.
Developers note that the system provides a means to monitor for bias:
[I]f we look at the child protection caseload, we can see, for example, what the age distribution of the children are, I can see what the ethnicity distribution for this client is, I can see what all those different characteristics are, what the deprivation is.  For each of those distributions, I can say what does the model do, is it a different distribution to the distribution in the client’s caseload? If it’s significantly different and the model is skewing oddly, why has it got a different bias to what’s naturally in the data? This could be because there is bias the client’s existing system or that the model is biased? So for the first time, I think, we can actually start looking at bias in the system, which is actually quite a powerful thing to be enabling. (developer)
This type of comparison will not address the extent to which other biases may enter a system, as argued by people like Virginia Eubanks, through assumptions about what is normal and what a family should look and act like.
Challenges

One of the challenges identified by developers interviewed is that others will not differentiate between the system they have developed and other uses of predictive analytics in child welfare in other countries.

Xantura identified a range of challenges have been identified through their efforts to make use of predictive analytics in partnership with local authorities during a local government event. 

These are listed as inadequate resources, unwillingness to share data, data quality, anxiety
about job replacement or loss, and a failure to realise the savings promised. Each of these warrant more public debate. 

Concerns about privacy and the public’s attitudes to how systems like these might invade privacy are an ongoing concern. 

There is interest in expanding the uses of predictive analytics in child welfare and social services globally. EY is said to have a global team looking at this.

